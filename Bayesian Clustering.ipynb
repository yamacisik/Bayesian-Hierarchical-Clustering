{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means vs Hierarchical Clustering vs Bayesian Hierarchical Clustering\n",
    "\n",
    "As an unsupervised learning model, for a given set of features a clustering model tries to form groups of a given number ( usually defined as k) from the data set. \n",
    "\n",
    "Both **k-means** and **hierarchical clustering** use a distance metric such as Euclidan Distance or Squared Euclidian Distance to form the clusters.\n",
    "\n",
    "**K-means**  starts with initial K-center mean points and assigns each observations to the center point with the lowest distance metric and then further updates the means based on the centroids of each assigned clusters.\n",
    "\n",
    "**Hierarchical Clustering** model initiliazes a node for each point, finds two nodes with the lowest distance and joins these nodes untill there is one node or k nodes left. This allows us to work with a tree form structure.\n",
    "\n",
    "**Bayesian Hierarchical Clustering** uses the same iterative process as standard hierarchical clustering but rather then using a distance metric, it uses a probability function that is calculated based on the Hypothesis that two nodes belongs to the same cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Hierarchical Clustering\n",
    "\n",
    "## Probability Calculations\n",
    "\n",
    "A brief explanations for the probability calculations are provided here, for further explanation please see :http://www2.stat.duke.edu/~kheller/bhcnew.pdf and https://github.com/mtw25/Stat663FinalProject-Matt_Yamac/blob/master/Final_Project_STA_663%20(1).pdf\n",
    "\n",
    "For each step, we find the the maximum \\\\(r_k\\\\) value among all possible combinations of nodes \\\\(i,j\\\\). The given nodes with the maximum value are then combined the create a new node. \n",
    "\n",
    "\\\\(r_k\\\\) value is calculated as follows:\n",
    "\n",
    "$$r_k= \\frac{\\pi_k p(D_k|H_1^k)}{p(D_k|T_k)}$$ \n",
    " \n",
    "\n",
    "$$p(D_k|T_k) = \\pi_k p(D_k|H_1^k)+(1-\\pi_k)p(D_i|T_i)p(D_j|T_j) $$\n",
    "\n",
    "\n",
    "$$ p(D_k|H_1^k)=\\frac{1}{\\pi^{nd/2}} \\frac{\\Gamma_d(v_n/2)}{\\Gamma_d(v_0/2)} \\frac{|\\Lambda_0|^{v_0/2}}{|\\Lambda_n|^{v_n/2}} (\\frac{\\kappa_0}{\\kappa_n})^{d/2}  $$\n",
    "\n",
    "where:\\\\(\\kappa_n= \\kappa_0 +n  , v_n = v_0 +n \\\\) and \\\\(\\Gamma_d\\\\) is the  Cumilative Distribution Function of multivariate Gamma Distrubition\n",
    "\n",
    "\\\\(\\kappa_0 , v_0 ,\\mu_0 , \\Lambda_0,\\alpha \\\\) are parameters to be initialized they will be discussed further. Specifically, \\\\(\\kappa_0 , v_0 ,\\mu_0 , \\Lambda_0,\\\\) are the parameters of Normal Inverse Wishar prior.\n",
    "\n",
    "We start by initializing $$\\pi_i= \\alpha \\\\ d_i=1$$ \n",
    "\n",
    "And update $$d_k=\\alpha\\Gamma(n_k)+ d_id_j$$  $$\\pi_k=\\alpha\\frac{\\Gamma(n_k)}{d_k}$$\n",
    "\n",
    "At each iterative step \\\\(r_k\\\\) values are calculated as explained above and new clusters are formed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Algorithm\n",
    "\n",
    "This implementation uses a tree structure defined as Node. Each Node has the following instance variables:\n",
    "- points: A set that represents each observation from the dataset.\n",
    "- d: \\\\(d_i\\\\) value described above, initialized as \\\\(\\alpha\\\\)\n",
    "- number: Unique number that identifies each node\n",
    "- left,right: Respective left and right nodes of each node.\n",
    "- ph: Probability of points being in the cluster, initialize to 0\n",
    "- pit, rit: Dictionaries for keeping track of \\\\((r_k\\\\) and \\\\(\\pi_k\\\\)) values of each potential combinations. Note that this varibles were added for the fast version of the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import factorial,multigammaln\n",
    "from decimal import Decimal\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define a new class called node which has the data points d_k and the number of the clusters\n",
    "\n",
    "\n",
    "class Node:\n",
    "\n",
    "\n",
    "    \n",
    "    def __init__(self,p,alpha,i):\n",
    "        \n",
    "        self.single=True\n",
    "        self.points=set()\n",
    "        self.points.add(p)\n",
    "        self.d=alpha\n",
    "        self.number=i\n",
    "        self.left=None\n",
    "        self.right=None\n",
    "        self.ph=0\n",
    "## Add list that holds probability with other nodes\n",
    "        self.pit={}\n",
    "        self.rit={}\n",
    "\n",
    "\n",
    "    def add(self,x):\n",
    "        self.points.add(x)\n",
    "        \n",
    "    def add_all(self,x):\n",
    "        self.points=x\n",
    "        \n",
    "    def remove(self,x):\n",
    "        self.points.remove(x)\n",
    "        \n",
    "    def combine(self,y,alpha,i=None):\n",
    "        ## Unless given a number set the number to the initial node\n",
    "        p=self.points.union(y.points)\n",
    "        z=Node(1,self.d,self.number)\n",
    "        if i!=None:\n",
    "            z.number=i\n",
    "        z.left=self\n",
    "        z.right=y\n",
    "        z.remove(1)\n",
    "        z.points=p\n",
    "        z.d= alpha*factorial(len(p)-1)+self.d*y.d\n",
    "        z.single=False\n",
    "        return z \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Function - Original Version\n",
    "This function uses two numpy matrices to calculate and update the \\\\((r_k\\\\) and \\\\(\\pi_k\\\\)) after each iteration. It is computationally really slow and has a complexity of \\\\(O(n^3)\\\\)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function that calculates the probability of the hyphothesis that the nodes belongs to the same cluster\n",
    "        \n",
    "def prob_hypo(X,kappa0,v0,mu0,eta0):\n",
    "    from decimal import Decimal\n",
    "    nf,df= X.shape\n",
    "    n=Decimal(nf)\n",
    "    d=Decimal(df)\n",
    "    a= (1/(Decimal(np.pi))**(n*d/2))\n",
    "    b=Decimal(multigammaln((v0+nf)/2,df)/multigammaln(v0/2,df))\n",
    "    S=np.zeros((df,df))\n",
    "    for i in range(nf):\n",
    "        o=X[i]-X.mean(axis=0)\n",
    "        S+=np.outer(o,o)\n",
    "    etan=(eta0) + S + (kappa0*nf/(kappa0+nf))*np.outer((X.mean(axis=0)-mu0),(X.mean(axis=0)-mu0))\n",
    "    c=Decimal(np.linalg.det(eta0)**(v0/2))/(Decimal(np.linalg.det(etan))**((Decimal(v0)+n)/2))\n",
    "    d=Decimal(kappa0/(kappa0+nf))**(d/2)\n",
    "    return float(a*b*c*d)\n",
    "\n",
    "## Function that initiates clusters from the given data set and the parameters for the prior\n",
    "def init(X,alpha,kappa0,v0,mu0,eta0):\n",
    "    x=[]\n",
    "    for i in range(len(X)):\n",
    "        node=Node(i,alpha,i)\n",
    "        node.ph=prob_hypo(X[[i]],kappa0,v0,mu0,eta0)\n",
    "        \n",
    "        x.append(node)\n",
    "        \n",
    "    return x\n",
    "\n",
    "\n",
    "## Function that calculates P and R values between each nodes and puts them into two numpy matrices\n",
    "## @ nodes,list of nodes taken from the data set, X = data set\n",
    "\n",
    "def calculate_r(nodes,alpha,X,kappa0,v0,mu0,eta0):\n",
    "    from scipy.special import factorial\n",
    "    n=len(nodes)\n",
    "    rik=np.zeros((n,n))\n",
    "    pit=np.zeros((n,n))\n",
    "    for i in range(n):\n",
    "        for j in range(i+1,n):\n",
    "            pit[i,j],rik[i,j]= get_pi_ri(nodes[i],nodes[j],alpha)\n",
    "            \n",
    "    np.fill_diagonal(rik,-10)\n",
    "    return rik,pit\n",
    "\n",
    "## Function for updating clusters, by creating a new node (combination of i and j) and deleting nodes i,j\n",
    "def update_clust(rk,pit,nodes,alpha):\n",
    "    i,j=np.unravel_index(np.argmax(rk),rk.shape)\n",
    "    if len(nodes[i].points)>len(nodes[j].points):\n",
    "        new_node=nodes[i].combine(nodes[j],alpha)\n",
    "    else:\n",
    "        new_node=nodes[j].combine(nodes[i],alpha)\n",
    "    new_node.single=False\n",
    "    new_node.ph=pit[i,j]\n",
    "    nodes[i]=new_node \n",
    "    del nodes[j]\n",
    "    return nodes\n",
    "\n",
    "\n",
    "## Final function, returns y_hat and the root of the tree\n",
    "def clust(X,alpha,mu0,eta0,kappa0=1,v0=2.5,k=3):\n",
    "    nodes=init(X,alpha,kappa0,v0,mu0,eta0)\n",
    "   ## Loop until n-k and set the cluster\n",
    "    for i in range(len(nodes)-k):\n",
    "        rk,pit=calculate_r(nodes,alpha,X,kappa0, v0,mu0, eta0)\n",
    "        nodes=update_clust(rk,pit,nodes,alpha)\n",
    "    y=np.zeros(len(X),dtype=\"int32\")\n",
    "    for i in range(k):\n",
    "        ind=list(nodes[i].points)\n",
    "        y[ind]=i\n",
    "    ## Finish the loop to output the final root \n",
    "    for i in range(k-1):\n",
    "        rk,pit=calculate_r(nodes,alpha,X,kappa0,v0,mu0,eta0)\n",
    "        nodes=update_clust(rk,pit,nodes,alpha)\n",
    "        \n",
    "    return y,nodes[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Function-Fast Version\n",
    "\n",
    "This version replaces the numpy matrcies with two dictionaries defined for each node.Rather creating a numpy matrix after updating the nodes, this version calculates the initial \\\\((r_k\\\\) , \\\\(\\pi_k\\\\)) for each point and keeps updating them after forming a new cluster. This results in a faster and efficient calculation. The run time complexity is reduced from \\\\(O(n^3)\\\\) to \\\\(O(n^2)\\\\)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function that calculates the probability of points being in the same cluster\n",
    "        \n",
    "def prob_hypo(X,kappa0,v0,mu0,eta0):\n",
    "    from decimal import Decimal\n",
    "    nf,df= X.shape\n",
    "    n=Decimal(nf)\n",
    "    d=Decimal(df)\n",
    "    a= (1/(Decimal(np.pi))**(n*d/2))\n",
    "    b=Decimal(multigammaln((v0+nf)/2,df)/multigammaln(v0/2,df))\n",
    "    S=np.zeros((df,df))\n",
    "    for i in range(nf):\n",
    "        o=X[i]-X.mean(axis=0)\n",
    "        S+=np.outer(o,o)\n",
    "    etan=(eta0) + S + (kappa0*nf/(kappa0+nf))*np.outer((X.mean(axis=0)-mu0),(X.mean(axis=0)-mu0))\n",
    "    c=Decimal(np.linalg.det(eta0)**(v0/2))/(Decimal(np.linalg.det(etan))**((Decimal(v0)+n)/2))\n",
    "    d=Decimal(kappa0/(kappa0+nf))**(d/2)\n",
    "    return float(a*b*c*d)\n",
    "\n",
    "## PIT, RIT calculatios between two nodes\n",
    "def get_pi_ri(i,j,alpha):\n",
    "    clust_k=i.combine(j,alpha)\n",
    "    nk=len(clust_k.points)\n",
    "    dk=clust_k.d\n",
    "    pi=alpha*factorial(nk-1)/dk \n",
    "    all_points=list(clust_k.points)\n",
    "    ph=prob_hypo(X[all_points][:],kappa0,v0,mu0,eta0) \n",
    "    pit=ph*pi+ (1-pi)*i.ph*j.ph\n",
    "    rit=(pi*ph)/pit\n",
    "    return pit,rit\n",
    "    \n",
    "    \n",
    "## Function that calculates the maximum value in a dictionary, return key and max value\n",
    "def get_dict_max(d):\n",
    "    ind=0\n",
    "    m=0\n",
    "    for i in d:\n",
    "        if d[i]>=m:\n",
    "            m=d[i]\n",
    "            ind=i\n",
    "    return ind,m\n",
    "\n",
    "## Get Node with the specific number from a list of nodes\n",
    "\n",
    "def get_node(i,nodes):\n",
    "    for node in nodes:\n",
    "        if node.number==i:\n",
    "            return node\n",
    "    \n",
    "## Incorporate pi,ri calculations go over all the nodes one inital time\n",
    "def init2(X,alpha,kappa0,v0,mu0,eta0):\n",
    "    x=[]\n",
    "    for i in range(len(X)):\n",
    "        node=Node(i,alpha,i)\n",
    "        node.ph=prob_hypo(X[[i]],kappa0,v0,mu0,eta0)\n",
    "        \n",
    "        x.append(node)\n",
    "    \n",
    "    from scipy.special import factorial\n",
    "    n=len(x)\n",
    "    for i in range(n):\n",
    "        for j in range(i+1,n):\n",
    "            p,r=get_pi_ri(x[i],x[j],alpha)\n",
    "            x[i].pit[j]=p\n",
    "            x[i].rit[j]=r  \n",
    "        \n",
    "        \n",
    "    return x\n",
    "\n",
    "## Function that creates a new node and drops the nodes that give the highest pit value. Add the pit,ri values to the new node\n",
    "\n",
    "def change_nodes(i,j,n,nodes,alpha):\n",
    " \n",
    "    n1=get_node(i,nodes)\n",
    "    n2=get_node(j,nodes)\n",
    "\n",
    "    new_node=n1.combine(n2,alpha,i=n)\n",
    "    new_node.ph=get_pi_ri(n1,n2,alpha=alpha)[0] \n",
    "    nodes.remove(n1)\n",
    "    nodes.remove(n2)\n",
    "    for node in nodes:\n",
    "        \n",
    "        new_node.pit[node.number],new_node.rit[node.number]=get_pi_ri(node,new_node,alpha=alpha)   \n",
    "        ## nodes[k].pit[n],nodes[k].rit[n] =get_pi_ri(nodes[k],new_node,alpha=alpha) \n",
    "        if n1.number in node.rit:\n",
    "            del node.rit[n1.number]\n",
    "        if  n2.number in node.rit:\n",
    "            del node.rit[n2.number]\n",
    "    nodes.append(new_node)\n",
    "    return nodes \n",
    "\n",
    "## Function for updating clusters, by creating a new node (combination of i and j) and deleting nodes i,j \n",
    "## and updating the probability values of each dictionary\n",
    "def update_clust2(nodes,n,alpha):    \n",
    "    ## Find the maximum rit\n",
    "    m=0\n",
    "    i=0\n",
    "    j=0\n",
    "    for k in range(len(nodes)):\n",
    "        cind,cm=get_dict_max(nodes[k].rit)\n",
    "        if cm>=m:\n",
    "            m=cm\n",
    "            i=nodes[k].number\n",
    "            j=cind\n",
    "    return change_nodes(i,j,n,nodes,alpha=alpha)\n",
    " \n",
    "\n",
    "def clust2(X,alpha,mu0,eta0,kappa0=1,v0=2.5,k=3):\n",
    "    n=len(X)\n",
    "    nodes=init2(X,alpha,kappa0,v0,mu0,eta0)\n",
    "    y=np.zeros((n))\n",
    "    for i in range(n-k):\n",
    "        nodes=update_clust2(nodes,n+i,alpha)\n",
    "    ## Set the labels\n",
    "    y=np.zeros(len(X),dtype=\"int32\")\n",
    "    for i in range(k):\n",
    "        ind=list(nodes[i].points)\n",
    "        y[ind]=i\n",
    "    ## Finish the classification and produce the table\n",
    "    for i in range(k-1) :\n",
    "        nodes=update_clust2(nodes,n+i,alpha)\n",
    "    return y,nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
